#   geom_hline(yintercept = fdr_thresh) +
#   scale_color_identity(labels = c(blue = "most fingerprintable", red = "least fingerprintable", gray = "all other subjects"),guide = "legend") +
#   mytheme
#
# s_ts_pval_bystim <- ggplot(df_long_pval, aes(x = stimulus, group = fingerprint, fill = color)) +
#   geom_histogram(stat = 'count') +
#   scale_fill_identity(labels = c(blue = "n subs fingerprint", gray = "n subs no fingerprint"), guide = "legend") +
#   mytheme
#
# ts_pval_bysub <- ggplot(df_long_pval, aes(x = variable, group = fingerprint, fill = color)) +
#   geom_histogram(stat = 'count') +
#   scale_fill_identity(labels = c(blue = "n frames fingerprint", gray = "n frames no fingerprint"), guide = "legend") +
#   mytheme + coord_flip()
}
}
# Plot thresholded ADF results
for (i_datapath in 1:nrow(datapaths_df)){
if (datapaths_df[i_datapath, "use"] == "Y"){
name_of_vid = datapaths_df[i_datapath, "video_name"]
NAME_adf_05 = paste0(name_of_vid, "_adf_thresh_05")
adf_results_05 = get(NAME_adf_05)
NAME_adf_01 = paste0(name_of_vid, "_adf_thresh_01")
adf_results_01 = get(NAME_adf_01)
NAME_adf_001 = paste0(name_of_vid, "_adf_thresh_001")
adf_results_001 = get(NAME_adf_001)
# plot heatmap of classifier output matrix of subjects by stimuli matrix
print(heatmap(as.matrix(adf_thresh_0.05),
hclustfun=function(d) hclust(d, method=clust_method),
scale = "none"))
print(heatmap(as.matrix(adf_thresh_0.01),
hclustfun=function(d) hclust(d, method=clust_method),
scale = "none"))
print(heatmap(as.matrix(adf_thresh_0.001),
hclustfun=function(d) hclust(d, method=clust_method),
scale = "none"))
}
}
View(despicable_me_adf_thresh_05)
knitr::opts_chunk$set(echo = TRUE)
#clean the environment before starting
rm(list=ls())
# function to create stimulus ids
get_stim_ids <- function(name_of_vid){
results_perStim = paste0(name_of_vid, "_results_perStim")
results_perStim = get(results_perStim)
# grab number of stimuli (frames) from the summary results by stimuli
nstimuli = nrow(results_perStim)
# add the vidname in case we concatenate videos
frame = paste0(name_of_vid, "_%03d")
#stim_names = character(length=nstimuli)
stim_ids = character(length=nstimuli)
for (i in 1:nstimuli){
#stim_names[i] = sprintf("frame%03d",i)
stim_ids[i] = sprintf(frame,i)
}
return(stim_ids)
} # function get_stim_ids
# function to clean up dataframes and compute missing data
clean_gaze_data <- function(nfpstim_per_subject_original, intrasubR_sub_by_stim){
#calculate number of complete obs
nfpstim_per_subject_original$missing_tot <- nstimuli - rowSums(is.na(intrasubR_sub_by_stim))
#divide complete obs by total number of stimuli
nfpstim_per_subject_original$missing_ratio <- (nfpstim_per_subject_original$missing_tot/nstimuli)
#multiple the complete obs to stimuli number ratio by the number of fingerprintable stimuli
#(this will reduce the count of fp stimuli for participants with missing data)
nfpstim_per_subject_original <- nfpstim_per_subject_original %>%
mutate(fpstimuli_ratioed = (round(missing_ratio*nfingerprintable_stimuli, 0)))
return(nfpstim_per_subject_original)
} # function clean_gaze_data
create_fp_df <- function(nfpstim_per_subject_original){
#create a new dataframe that contains the weighted number of fp stimuli
nfpstim_per_subject <- data.frame(nfpstim_per_subject_original$fpstimuli_ratioed)
rownames(nfpstim_per_subject) <- rownames(nfpstim_per_subject_original)
nfpstim_per_subject <- nfpstim_per_subject %>%
rename(nfingerprintable_stimuli = nfpstim_per_subject_original.fpstimuli_ratioed)
return(nfpstim_per_subject)
} # function create_fp_df
# get complete data
get_complete_data <- function(intrasubR_sub_by_stim, pheno_data, nstimuli){
na_mask = rowSums(is.na(intrasubR_sub_by_stim))>(nstimuli*0.15)
subs2exclude = rownames(intrasubR_sub_by_stim)[na_mask]
mask1 = is.element(pheno_data$subj_ID,rownames(intrasubR_sub_by_stim))
mask2 = !is.element(pheno_data$subj_ID, subs2exclude)
mask = mask1 & mask2
return(mask)
} # function get_complete_data
# function to do gaze fingerprint classification for averaged stimuli
# (as in Keeles et al.,)
gaze_fingerprint_classifier <- function(file2use, na_mask, nperm=0){
#tmp_data = read.csv(file2use, row.names=1)
tmp_data = file2use
tmp_data = tmp_data[!na_mask,!na_mask]
tmp_res = matrix(nrow = dim(tmp_data)[1], ncol=3)
for (isub in 1:dim(tmp_data)[1]){
tmp_res[isub,1] = isub
tmp_res[isub,2] = which(tmp_data[isub,]==max(tmp_data[isub,]))
}
tmp_res[,3] = tmp_res[,1]==tmp_res[,2]
accuracy = sum(tmp_res[,3])/dim(tmp_data)[1]
result = data.frame(matrix(nrow = 1, ncol = 3))
colnames(result) = c("accuracy", "mean_null", "pval")
result$accuracy = accuracy
if (nperm>0){
tmp_perm_res = data.frame(matrix(nrow = nperm, ncol = 2))
colnames(tmp_perm_res) = c("perm_num","accuracy")
for (iperm in 1:nperm){
# print(iperm)
subids = rownames(tmp_data)
set.seed(iperm)
rand_perm = sample(length(subids))
perm_tmp_data = tmp_data[subids[rand_perm],]
perm_tmp_res = matrix(nrow = dim(perm_tmp_data)[1], ncol=3)
for (isub in 1:dim(perm_tmp_data)[1]){
perm_tmp_res[isub,1] = isub
perm_tmp_res[isub,2] = which(perm_tmp_data[isub,]==max(perm_tmp_data[isub,]))
}
perm_tmp_res[,3] = perm_tmp_res[,1]==perm_tmp_res[,2]
perm_accuracy = sum(perm_tmp_res[,3])/dim(perm_tmp_data)[1]
tmp_perm_res[iperm, "perm_num"] = iperm
tmp_perm_res[iperm, "accuracy"] = perm_accuracy
}
p_value = (sum(tmp_perm_res$accuracy>=accuracy)+1)/(nperm+1)
result$pval = p_value
result$mean_null = mean(tmp_perm_res$accuracy)
}
return(result)
} # function gaze_fingerprint_classifier
run_gaze_fingerprint_classifier <- function(file2use, intra_sub_file, pheno_data_sub){
# file2use = despicable_me_id_mat_all
# intra_sub_file = despicable_me_intrasubR_sub_by_stim
# pheno_data_sub
# subjects to remove because of too many NAs
na_mask = is.element(rownames(intra_sub_file),pheno_data_sub$subj_ID)
gfp_res = data.frame(matrix(nrow = 1, ncol = 3))
colnames(gfp_res) = c("accuracy", "mean_null", "pval")
rownames(gfp_res) = "mean_stim"
nperm = 1000#0
gfp_res[1,] = gaze_fingerprint_classifier(file2use, na_mask, nperm=nperm)
return(gfp_res)
} # function run_gaze_fingerprint_classifier
compute_pval <- function(classifier_output_sub_by_stim,
classifier_output_stim_by_perm,
pheno_data_sub,
stim_ids, site, nstimuli, fdr_thresh){
# subjects to remove because of too many NAs
na_mask = is.element(rownames(classifier_output_sub_by_stim),pheno_data_sub$subj_ID)
df_res = classifier_output_sub_by_stim
df_res = df_res[!na_mask,]
df_res_perm = classifier_output_stim_by_perm
nperm = ncol(df_res_perm)
subs2use = rownames(df_res)
df2plot = data.frame(stim_ids = stim_ids,
accuracy = colSums(df_res[,stim_ids])/dim(df_res)[1],
site=site)
df2plot$site = factor(df2plot$site)
# calculate p-values based on permutation accuracies
for (istim in 1:nstimuli){
df2plot$pval[istim] =
(sum(df_res_perm[istim,]>=df2plot$accuracy[istim])+1)/(nperm+1)
}
# calculate FDR
df2plot$fdr = p.adjust(df2plot$pval, method = "fdr")
df2plot$fingerprint = "No"
df2plot$fingerprint[df2plot$fdr<=fdr_thresh] = "Yes"
return(df2plot)
} # compute_pval
fp_res_df <- function(pheno_data_sub, nfpstim_per_subject, nfpstim_sub_by_perm){
fp_res = nfpstim_per_subject
fp_res$subids = rownames(fp_res)
na_mask = is.element(rownames(nfpstim_per_subject),pheno_data_sub$subj_ID)
fp_res = fp_res %>% filter(na_mask)
fp_perm_res = nfpstim_sub_by_perm
na_mask = is.element(rownames(nfpstim_sub_by_perm),pheno_data_sub$subj_ID)
fp_perm_res = fp_perm_res %>% filter(na_mask)
nperm = ncol(fp_perm_res)
# figure out p-values for each subject based on permutation nfingerprintable stim
for (isub in 1:dim(fp_res)[1]){
fp_res$pval[isub] = (sum(fp_perm_res[isub,]>=fp_res$nfingerprintable_stimuli[isub])+1)/(nperm+1)
}
fp_res$fdr = p.adjust(fp_res$pval, method = "fdr")
fp_res$Fingerprintable = "Yes"
fp_res$Fingerprintable[fp_res$fdr>fdr_thresh] = "No"
reorder_vect = order(fp_res[,"nfingerprintable_stimuli"])
ordered_fp_res_subs = rownames(fp_res)[reorder_vect]
#ordered_fp_res = fp_res[ordered_fp_res_subs,]
fp_res$subids = factor(fp_res$subids, levels = ordered_fp_res_subs)
return(fp_res)
}
fp_perm_res_df <- function(fp_res, pheno_data_sub, nfpstim_sub_by_perm){
fp_perm_res = nfpstim_sub_by_perm
na_mask = is.element(rownames(nfpstim_sub_by_perm),pheno_data_sub$subj_ID)
fp_perm_res = fp_perm_res %>% filter(na_mask)
fp_perm_res$Fingerprintable = "Yes"
fp_perm_res$Fingerprintable[fp_res$fdr>fdr_thresh] = "No"
fp_perm_res$subids = rownames(fp_res)
#fp_perm_res$subids = factor(fp_perm_res$subids, levels = ordered_fp_res_subs)
return(fp_perm_res)
}
gui_res_df <- function(pheno_data_sub, fpratio_sub_by_stim, fdr_thresh){
# make ggridges plot for gaze uniqueness index (GUI; aka fingerprint ratio)
# test each subject for whether GUI is greater than the null value of 0 after log10 transformation
# (because GUI is positively skeweed).
# This will allow you to identify which subjects have an overall GUI
# across all stimuli greater than the null value of 0.
na_mask = is.element(rownames(fpratio_sub_by_stim), pheno_data_sub$subj_ID)
tmp_gui = fpratio_sub_by_stim %>% filter(na_mask)
tmp_gui = t(tmp_gui)
gui_res_sub = data.frame(matrix(nrow = dim(tmp_gui)[2], ncol = 4))
colnames(gui_res_sub) = c("subid","tstat","pval","fdr")
rownames(gui_res_sub) = colnames(tmp_gui)
subids2use = colnames(tmp_gui)
for (sub in subids2use){
gui_res_sub[sub, "subid"] = sub
tres = t.test(log10(tmp_gui[,sub]), mu = 0)
gui_res_sub[sub, "tstat"] = tres$statistic[[1]]
gui_res_sub[sub, "pval"] = tres$p.value[[1]]
}
gui_res_sub$fdr = p.adjust(gui_res_sub$pval, method = "fdr")
gui_res_sub$Unique = "Yes"
# anything with tstat<0 of fdr>0.05 is not unique
gui_res_sub$Unique[gui_res_sub$tstat<0 | gui_res_sub$fdr>0.05] = "No"
gui_res_sub = gui_res_sub[order(gui_res_sub$tstat),]
gui_res_sub$subid = factor(gui_res_sub$subid, levels = gui_res_sub$subid)
gui_res_sub = gui_res_sub[order(-gui_res_sub$tstat),]
gui_res_sub$uniqueness_rank = c(1:dim(gui_res_sub)[1]) # make gaze uniqueness ranking based on strength of tstat
gui_res = fpratio_sub_by_stim %>% filter(na_mask)
gui_res$subid = rownames(gui_res)
gui_res$subid = factor(gui_res$subid, levels = rev(gui_res_sub$subid))
gui_res = merge(gui_res, gui_res_sub[,c("subid","Unique")])
return(gui_res)
}
sub_by_sub_adf <- function(accuracy_df){
# for every row pair, compute a linear model
adf_result_df = data.frame()
for (i in 1:ncol(accuracy_df)){
for (j in 1:ncol(accuracy_df)){
var1 = accuracy_df[, i]
var2 = accuracy_df[, j]
# compute linear regression (OLS)
model_result = lm(var1 ~ var2)
# take residuals
residuals = model_result$residuals
# compute ADF statistic
adf_result = adf(residuals)
p_value = adf_result$p.value
adf_result_df[i, j] = p_value
}
}
rownames(adf_result_df) = colnames(accuracy_df)
colnames(adf_result_df) = colnames(accuracy_df)
# convert diagonals to NAs
diag(adf_result_df) <- NA
return(adf_result_df)
}
adf_threshold <- function(adf_result_df, thresh){
adf_sig_matrix = data.frame()
for (i in 1:ncol(adf_result_df)){
col2compare = adf_result_df[i, ]
# if p-value higher than threshold, assign 0
# otherwise assign 1
res <- ifelse(col2compare > thresh, 0, 1)
adf_sig_matrix <- rbind(adf_sig_matrix, res)
}
diag(adf_sig_matrix) <- 1
return(adf_sig_matrix)
}
#link to download impute into updated R version (regular way won't work with R vs > 4)
#https://www.bioconductor.org/packages/release/bioc/html/impute.html
# ADF test: https://search.r-project.org/CRAN/refmans/bootUR/html/adf.html
library(easypackages)
libraries("here","ggplot2","ggridges","reshape2","patchwork","RColorBrewer",
"plotrix","tidyverse","cluster","ggpackets","wordcloud","proxy",
"ggeasy","NbClust","MASS","robustbase","MKinfer","impute","ggheatmap",
"janitor", "lme4", "lmtest","matlabr","psych", "bootUR")
# PLEASE DO NOT CHANGE TO here() or setwd() or some other variation
# PATHS ARE HARDCODED FOR REPRODUCIBILITY
# hard code the paths, need to be changes if running on a different machine
codepath = "/Users/scrockford/Library/CloudStorage/OneDrive-FondazioneIstitutoItalianoTecnologia/gazeprint_videos/analysis/all_analysis/code"
resultpath = "/Users/scrockford/Library/CloudStorage/OneDrive-FondazioneIstitutoItalianoTecnologia/gazeprint_videos/analysis/all_analysis/results"
plot_path = "/Users/scrockford/Library/CloudStorage/OneDrive-FondazioneIstitutoItalianoTecnologia/gazeprint_videos/analysis/all_analysis/plots"
source(file.path(codepath,"utils_LOMBARDO.R"))
source(file.path(codepath, "utils_CROCKFORD.R"))
# hard coded paths to the results files from each individual video
despicablepath = "/Users/scrockford/Library/CloudStorage/OneDrive-FondazioneIstitutoItalianoTecnologia/gazeprint_videos/analysis/despicable_me/data"
diarypath = "/Users/scrockford/Library/CloudStorage/OneDrive-FondazioneIstitutoItalianoTecnologia/gazeprint_videos/analysis/diary_of_a_wimpy_kid/data"
presentpath = "/Users/scrockford/Library/CloudStorage/OneDrive-FondazioneIstitutoItalianoTecnologia/gazeprint_videos/analysis/the_present/data"
fractalspath = "/Users/scrockford/Library/CloudStorage/OneDrive-FondazioneIstitutoItalianoTecnologia/gazeprint_videos/analysis/fun_with_fractals/data"
totorosocialpath = "/Users/scrockford/Library/CloudStorage/OneDrive-FondazioneIstitutoItalianoTecnologia/gazeprint_videos/analysis/totoro_social/data"
totorononsocialpath = "/Users/scrockford/Library/CloudStorage/OneDrive-FondazioneIstitutoItalianoTecnologia/gazeprint_videos/analysis/totoro_nonsocial/data"
# data-frame of all the results paths
datapaths_df <- data.frame("datapath" = c(despicablepath,
diarypath,
presentpath,
fractalspath,
totorononsocialpath,
totorosocialpath),
"video_name" = c("despicable_me",
"diary_of_a_wimpy_kid",
"the_present",
"fun_with_fractals",
"totoro_nonsocial",
"totoro_social"))
datapaths_df$use <- c("Y", "Y", "Y", "N", "N", "Y")
# set some hard coded variables (plot titles, p-value adjustment threshold)
plot_title_name = "IIT"
site = "IIT"
WEIGHT = "none"
#method we will use for clustering
clust_method = "ward.D2"
fdr_thresh = 0.05
# Grab some important information from one the dataframes (subject ids, n of frames)
###Read in phenotypic data
pheno_data = read.csv("/Users/scrockford/Library/CloudStorage/OneDrive-FondazioneIstitutoItalianoTecnologia/gazeprint_videos/data/pheno/pheno_adult_videos.csv")
id_site_df = data.frame(subid = pheno_data$subj_ID)
id_site_df$site = "IIT"
rownames(id_site_df) = id_site_df$subid
# number of permutations computed for gazefingerpint classifier and dynamic
nperm = 1000
# set seed for any randomization
set.seed(999)
# Load all files
for (i_datapath in 1:nrow(datapaths_df)) {
videoname <- datapaths_df$video_name[i_datapath]
datapath <- datapaths_df$datapath[i_datapath]
filenames <- list.files(path=datapath, pattern="+.*csv")
names2use <- sub(".csv", "", filenames)
# data-frame of new file names
filenames2use_df <- data.frame("filename" = names2use)
filenames2use_df$dfname <- c("ts_classification",
"ts_accuracy",
"classifier_output_stim_by_perm",
"classifier_output_sub_by_stim",
"nfpstim_sub_by_perm_original",
"nfpstim_per_subject_original",
"entropy_sub_by_sess",
"fpratio_sub_by_stim",
"id_mat_all",
"intrasubR_sub_by_stim",
"mean_fpratio_stim_by_perm",
"mean_intersubR_sub_by_stim",
"mean_intrasubR_perStim_perm",
"mean_intersubjectR_perStim_perm",
"results_perStim")
for(i_filename in 1:nrow(filenames2use_df)){
filename <- filenames2use_df$filename[i_filename]
dfname <- filenames2use_df$dfname[i_filename]
final_dfname <- paste(videoname, dfname, sep = "_")
filepath <- file.path(datapath, paste(filename,".csv",sep=""))
assign(final_dfname, read.csv(filepath, header = TRUE, row.names = 1, na.strings = "NaN", sep = ","))
}
}
# Clean up names for data frames
# DESPICABLE ME
# create stimulus ids
desp_stim_ids = get_stim_ids("despicable_me")
# clean up columnnames in dataframes
colnames(despicable_me_classifier_output_sub_by_stim) = desp_stim_ids
colnames(despicable_me_fpratio_sub_by_stim) = desp_stim_ids
colnames(despicable_me_intrasubR_sub_by_stim) = desp_stim_ids
colnames(despicable_me_mean_intersubR_sub_by_stim) = desp_stim_ids
rownames(despicable_me_ts_accuracy) = desp_stim_ids
rownames(despicable_me_ts_classification) = desp_stim_ids
# DIARY OF A WIMPY KID
diary_stim_ids = get_stim_ids("diary_of_a_wimpy_kid")
# clean up columnnames in dataframes
colnames(diary_of_a_wimpy_kid_classifier_output_sub_by_stim) = diary_stim_ids
colnames(diary_of_a_wimpy_kid_fpratio_sub_by_stim) = diary_stim_ids
colnames(diary_of_a_wimpy_kid_intrasubR_sub_by_stim) = diary_stim_ids
colnames(diary_of_a_wimpy_kid_mean_intersubR_sub_by_stim) = diary_stim_ids
rownames(diary_of_a_wimpy_kid_ts_accuracy) = diary_stim_ids
rownames(diary_of_a_wimpy_kid_ts_classification) = diary_stim_ids
# THE PRESENT
pres_stim_ids = get_stim_ids("the_present")
# clean up columnnames in dataframes
colnames(the_present_classifier_output_sub_by_stim) = pres_stim_ids
colnames(the_present_fpratio_sub_by_stim) = pres_stim_ids
colnames(the_present_intrasubR_sub_by_stim) = pres_stim_ids
colnames(the_present_mean_intersubR_sub_by_stim) = pres_stim_ids
rownames(the_present_ts_accuracy) = pres_stim_ids
rownames(the_present_ts_classification) = pres_stim_ids
# FUN WITH FRACTALS
fract_stim_ids = get_stim_ids("fun_with_fractals")
# clean up columnnames in dataframes
colnames(fun_with_fractals_classifier_output_sub_by_stim) = fract_stim_ids
colnames(fun_with_fractals_fpratio_sub_by_stim) = fract_stim_ids
colnames(fun_with_fractals_intrasubR_sub_by_stim) = fract_stim_ids
colnames(fun_with_fractals_mean_intersubR_sub_by_stim) = fract_stim_ids
rownames(fun_with_fractals_ts_accuracy) = fract_stim_ids
rownames(fun_with_fractals_ts_classification) = fract_stim_ids
# TOTORO NON SOCIAL
tot_nonsoc_stim_ids = get_stim_ids("totoro_nonsocial")
# clean up columnnames in dataframes
colnames(totoro_nonsocial_classifier_output_sub_by_stim) = tot_nonsoc_stim_ids
colnames(totoro_nonsocial_fpratio_sub_by_stim) = tot_nonsoc_stim_ids
colnames(totoro_nonsocial_intrasubR_sub_by_stim) = tot_nonsoc_stim_ids
colnames(totoro_nonsocial_mean_intersubR_sub_by_stim) = tot_nonsoc_stim_ids
rownames(totoro_nonsocial_ts_accuracy) = tot_nonsoc_stim_ids
rownames(totoro_nonsocial_ts_classification) = tot_nonsoc_stim_ids
# TOTORO SOCIAL
tot_soc_stim_ids = get_stim_ids("totoro_social")
# clean up columnnames in dataframes
colnames(totoro_social_classifier_output_sub_by_stim) = tot_soc_stim_ids
colnames(totoro_social_fpratio_sub_by_stim) = tot_soc_stim_ids
colnames(totoro_social_intrasubR_sub_by_stim) = tot_soc_stim_ids
colnames(totoro_social_mean_intersubR_sub_by_stim) = tot_soc_stim_ids
rownames(totoro_social_ts_accuracy) = tot_soc_stim_ids
rownames(totoro_social_ts_classification) = tot_soc_stim_ids
# get stimuli number
datapaths_df$nstimuli = NA
for (i_datapath in 1:nrow(datapaths_df)){
# get video name
name_of_vid = datapaths_df[i_datapath, "video_name"]
results_perStim = paste0(name_of_vid, "_results_perStim")
results_perStim = get(results_perStim)
# grab number of stimuli (frames) from the summary results by stimuli
nstimuli = nrow(results_perStim)
datapaths_df[i_datapath, "nstimuli"] = nstimuli
}
# Compute missing stim and add info to original df
# DESPICABLE ME
despicable_me_nfpstim_per_subject_original = clean_gaze_data(despicable_me_nfpstim_per_subject_original,
despicable_me_intrasubR_sub_by_stim)
despicable_me_nfpstim_per_subject = create_fp_df(despicable_me_nfpstim_per_subject_original)
# DIARY OF A WIMPY KID
diary_of_a_wimpy_kid_nfpstim_per_subject_original = clean_gaze_data(diary_of_a_wimpy_kid_nfpstim_per_subject_original,
diary_of_a_wimpy_kid_intrasubR_sub_by_stim)
diary_of_a_wimpy_kid_nfpstim_per_subject = create_fp_df(diary_of_a_wimpy_kid_nfpstim_per_subject_original)
# THE PRESENT
the_present_nfpstim_per_subject_original = clean_gaze_data(the_present_nfpstim_per_subject_original,
the_present_intrasubR_sub_by_stim)
the_present_nfpstim_per_subject = create_fp_df(the_present_nfpstim_per_subject_original)
# FUN WITH FRACTALS
fun_with_fractals_nfpstim_per_subject_original = clean_gaze_data(fun_with_fractals_nfpstim_per_subject_original,
fun_with_fractals_intrasubR_sub_by_stim)
fun_with_fractals_nfpstim_per_subject = create_fp_df(fun_with_fractals_nfpstim_per_subject_original)
# TOTORO NON SOCIAL
totoro_nonsocial_nfpstim_per_subject_original = clean_gaze_data(totoro_nonsocial_nfpstim_per_subject_original,
totoro_nonsocial_intrasubR_sub_by_stim)
totoro_nonsocial_nfpstim_per_subject = create_fp_df(totoro_nonsocial_nfpstim_per_subject_original)
# TOTORO SOCIAL
totoro_social_nfpstim_per_subject_original = clean_gaze_data(totoro_social_nfpstim_per_subject_original,
totoro_social_intrasubR_sub_by_stim)
totoro_social_nfpstim_per_subject = create_fp_df(totoro_social_nfpstim_per_subject_original)
# Clean up data and grab list of usable participants for each video
mask_desp_me = get_complete_data(despicable_me_intrasubR_sub_by_stim, pheno_data, datapaths_df[1, "nstimuli"])
mask_diary = get_complete_data(diary_of_a_wimpy_kid_intrasubR_sub_by_stim, pheno_data, datapaths_df[2, "nstimuli"])
mask_present = get_complete_data(the_present_intrasubR_sub_by_stim, pheno_data, datapaths_df[3, "nstimuli"])
#mask_fractals = get_complete_data(fun_with_fractals_intrasubR_sub_by_stim, pheno_data, datapaths_df[4, "nstimuli"])
#mask_tot_nonsocial = get_complete_data(totoro_nonsocial_intrasubR_sub_by_stim, pheno_data, datapaths_df[5, "nstimuli"])
mask_tot_social = get_complete_data(totoro_social_intrasubR_sub_by_stim, pheno_data, datapaths_df[6, "nstimuli"])
# fix the perm file
despicable_me_nfpstim_sub_by_perm = round(despicable_me_nfpstim_sub_by_perm_original*despicable_me_nfpstim_per_subject_original$missing_ratio, 0)
diary_of_a_wimpy_kid_nfpstim_sub_by_perm = round(diary_of_a_wimpy_kid_nfpstim_sub_by_perm_original*diary_of_a_wimpy_kid_nfpstim_per_subject_original$missing_ratio, 0)
the_present_nfpstim_sub_by_perm = round(the_present_nfpstim_sub_by_perm_original*the_present_nfpstim_per_subject_original$missing_ratio, 0)
fun_with_fractals_nfpstim_sub_by_perm = round(fun_with_fractals_nfpstim_sub_by_perm_original*fun_with_fractals_nfpstim_per_subject_original$missing_ratio, 0)
totoro_nonsocial_nfpstim_sub_by_perm = round(totoro_nonsocial_nfpstim_sub_by_perm_original*totoro_nonsocial_nfpstim_per_subject_original$missing_ratio, 0)
totoro_social_nfpstim_sub_by_perm = round(totoro_social_nfpstim_sub_by_perm_original*totoro_social_nfpstim_per_subject_original$missing_ratio, 0)
# take a final mask of subjetc ids of participants with a FULL SET OF COMPLETE DATA
final_mask = mask_desp_me & mask_diary & mask_present & mask_tot_social #& mask_fractals & mask_tot_nonsocial
pheno_data_sub = pheno_data %>% filter(final_mask)
# print out some descriptive stuff on the pheno data
table(pheno_data_sub$sex)
# get final number of subjects
n_subjects = nrow(pheno_data_sub)
for (i_datapath in 1:nrow(datapaths_df)){
if (datapaths_df[i_datapath, "use"] == "Y"){
name_of_vid = datapaths_df[i_datapath, "video_name"]
NAME_id_mat_df = paste0(name_of_vid, "_id_mat_all")
NAME_intrasubR_df = paste0(name_of_vid, "_intrasubR_sub_by_stim")
id_mat_all = get(NAME_id_mat_df)
intrasubR_sub_by_stim = get(NAME_intrasubR_df)
gfp_res = run_gaze_fingerprint_classifier(id_mat_all,
intrasubR_sub_by_stim,
pheno_data_sub)
message = paste0(name_of_vid, " ", "average gaze fingerprinting accuracy")
print(message)
print(gfp_res)
}
}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
#link to download impute into updated R version (regular way won't work with R vs > 4)
#https://www.bioconductor.org/packages/release/bioc/html/impute.html
library(easypackages)
#IF YOU HAVE ISSUES WITH RETICULATE:
#https://stackoverflow.com/questions/50145643/unable-to-change-python-path-in-reticulate
libraries("here","ggplot2","ggridges","reshape2","patchwork","RColorBrewer",
"plotrix","tidyverse","cluster","ggpackets","wordcloud","proxy",
"ggeasy","NbClust","MASS","robustbase","MKinfer","impute","ggheatmap",
"janitor", "lme4", "lmtest","matlabr","psych", "DescTools", "entropy",
"reticulate","graph4lg", "ade4", "metan", "cocor")
codepath = here("code")
resultpath = here("results")
datapath = here("data")
# Get filenames to read in
filepath <- file.path(resultpath, "results_15102024")
filenames <- list.files(path=filepath, pattern="+.*csv")
names2use <- sub(".csv", "", filenames)
for(i in names2use){
datafilepath <- file.path(filepath, paste(i,".csv",sep=""))
assign(i, read.csv(datafilepath, header = TRUE, na.strings = "NaN", sep = ","))
}
devtools::create("modelling_similarity")
devtools::create("similaritymodels")
pwd()
getwd()
cd R/
setwd('/Users/scrockford/similaritymodels/R')
getwd()
%in%
library(graph4lg)
library(similaritymodels)
